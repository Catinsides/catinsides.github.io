<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SCRAPY爬取智联招聘信息 | catinsides.github.io</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/css/highlight.css">

  
  <meta name="description" content="新的一年准备换一个新工作，如何快速获取大量的职位信息呢？干脆编写一个爬虫好了，顺带巩固一下知识。">
<meta property="og:type" content="article">
<meta property="og:title" content="SCRAPY爬取智联招聘信息">
<meta property="og:url" content="https://catinsides.github.io/2017/02/07/SCRAPY%E7%88%AC%E5%8F%96%E6%99%BA%E8%81%94%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF/index.html">
<meta property="og:site_name" content="catinsides.github.io">
<meta property="og:description" content="新的一年准备换一个新工作，如何快速获取大量的职位信息呢？干脆编写一个爬虫好了，顺带巩固一下知识。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://okzvvfkr0.bkt.clouddn.com/zhilian_urls.jpg">
<meta property="og:image" content="http://okzvvfkr0.bkt.clouddn.com/description.jpg">
<meta property="article:published_time" content="2017-02-07T05:41:00.000Z">
<meta property="article:modified_time" content="2024-03-28T02:36:58.462Z">
<meta property="article:author" content="catinsides">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://okzvvfkr0.bkt.clouddn.com/zhilian_urls.jpg"><meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="wrapper">
    <header id="header">
  <h1 id="title">
    <a href="/">catinsides.github.io</a>
  </h1>
  <nav>
    
    
      
      <a class="nav-link" href="/">Home</a>
    
      
        <span class="nav-spacer">×</span>
      
      <a class="nav-link" href="/archives">Archives</a>
    
      
        <span class="nav-spacer">×</span>
      
      <a class="nav-link" target="_blank" rel="noopener" href="https://github.com/Catinsides">Github</a>
    
    
  </nav>
</header>

    <div id="content">
      <article id="post-SCRAPY爬取智联招聘信息" class="article article-type-post" itemprop="blogPost" itemscope>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 class="article-title" itemprop="headline name">
      SCRAPY爬取智联招聘信息
    </h2>
  


        <div class="article-meta">
          <time class="article-date" datetime="2017-02-07T05:41:00.000Z" itemprop="datePublished">二月 7, 2017, 1:41 下午</time>

          
            × <span class="article-word-count">2k words</span>
            
            × <span class="article-time-to-read">8 minutes</span>
            
          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      
        <p>网上找工作这件事，首先要登录主站搜索职位名称，然后在一大串列表里逐个查看职位需求，查看完毕关闭页面，再打开新页面，想一想都觉得是费心费力的一件事。俗话说：“磨刀不误砍柴工”，再加上自己懒癌发作，心想着一定要一劳永逸的解决这件事。正好SCRAPY框架能派上用场，大致整理了一下思路开始撸代码。</p>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>根据文章开头部分的话，可以整理出<code>网上找工作</code>这一系列动作的流程</p>
<ol>
<li>打开主站搜索<code>职位</code>，并选择<code>地点</code></li>
<li>网站返回一系列<code>&lt;li&gt;&lt;a&gt;职位名称&lt;/a&gt;&lt;/li&gt;</code></li>
<li>点击<code>&lt;a&gt;</code>进入详情页，里面包含着职位和公司的详细信息</li>
<li>查看完毕后关闭，换下一个<code>&lt;a&gt;</code></li>
</ol>
<p>好了，流程整理好了，怎么实现呢？<br>显然第一步打开网站获取网页信息可以使用<code>urllib</code>或者<code>requests</code>模块，第二步提取<code>URL</code>信息交给<code>re</code>和<code>BeautifulSoup</code>处理，第三步使用<code>SCRAPY</code>处理第二步获取的<code>URL</code>到逐个页面提取职位详细信息，使用<code>SCRAPY</code>内置的<code>xpath</code>即可。<br>思路整理完毕，let’s coding!</p>
<h3 id="获取URL"><a href="#获取URL" class="headerlink" title="获取URL"></a>获取URL</h3><p>秉着模块化的精神，我把提取URL这一部分单独写一个文件，然后在<code>SCRAPY</code>调用。这样不仅方便调试，还能减少<code>SCRAPY</code>内爬虫文件代码的冗杂程度。<br>首先，引入必要的模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>为了在其他模组内调用，创建一个<code>Geturls</code>类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Geturls</span>():</span><br></pre></td></tr></table></figure>
<p>其他方法都在这里完成。<br>定义打开网页的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">open</span>(<span class="params">self, url</span>):</span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		request = urllib2.Request(url)</span><br><span class="line">		response = urllib2.urlopen(request)</span><br><span class="line">		content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">		<span class="keyword">return</span> content</span><br><span class="line">	<span class="keyword">except</span> urllib2.URLError, e:</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">hasattr</span>(e,<span class="string">&quot;code&quot;</span>):</span><br><span class="line">			<span class="built_in">print</span> e.code</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">hasattr</span>(e,<span class="string">&quot;reason&quot;</span>):</span><br><span class="line">			<span class="built_in">print</span> e.reason</span><br></pre></td></tr></table></figure>
<p>打开网页成功后将返回网页内所有的内容，然后提取关键信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">distill</span>(<span class="params">self, content</span>):</span><br><span class="line">		soup = BeautifulSoup(content, <span class="string">&#x27;html.parser&#x27;</span>).find_all(href=re.<span class="built_in">compile</span>(<span class="string">&quot;http://jobs.zhaopin.com/&quot;</span>))</span><br><span class="line">		urls_temp = soup[:<span class="number">60</span>]</span><br><span class="line">		urls = []</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> url <span class="keyword">in</span> urls_temp:</span><br><span class="line">			pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;href=&quot;(.*?)&quot;&#x27;</span>, re.S)</span><br><span class="line">			url_temp = re.findall(pattern, <span class="built_in">str</span>(url))</span><br><span class="line">			urls.append(url_temp)</span><br><span class="line">		<span class="keyword">return</span> urls</span><br></pre></td></tr></table></figure>
<p>原本想把方法名设置为<code>extract</code>，但是<code>SCRAPY</code>内有相同的方法名，只好换成<code>distill</code>(蒸馏)，我觉得反而更加形象XD.<br>智联网页上的东西很多，而我只需要职位的链接，所以使用正则把链接提出来即可。这些链接的规律就是开头皆为<code>http://jobs.zhaopin.com/</code>,后面附带一系列数字。查看网页源代码后，发现还是蛮有规律的，在第一链接之前没有其他的广告链接，而且每页的职位信息顺序排列只有60个，在这之后又会有11个广告链接，如下图</p>
<p><img src="http://okzvvfkr0.bkt.clouddn.com/zhilian_urls.jpg"></p>
<p>所以只需要提取出前60个即可。</p>
<h3 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h3><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>接下来的工作是，在上一步获得了所有的url之后，传到<code>SCRAPY</code>逐个打开页面获取职位信息。在目标目录内打开命令行执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject recruit</span><br></pre></td></tr></table></figure>
<p>好吧，这个项目名字不是很高大上，起码能够望文生义吧……<br>命令执行后，会自动生成下列文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|--recruit/</span><br><span class="line">|	|--scrapy.cfg</span><br><span class="line">|	|--recruit/</span><br><span class="line">|		|--`__init__.py`</span><br><span class="line">|		|--items.py</span><br><span class="line">|		|--piplines.py</span><br><span class="line">|		|--settings.py</span><br><span class="line">|		|--spiders/</span><br><span class="line">|		|--`__init__.py`</span><br></pre></td></tr></table></figure>
<p>将上一步完成的代码保存为<code>geturls.py</code>放到<code>recruit</code>(与items.py等文件同级)目录下。</p>
<h4 id="定义items"><a href="#定义items" class="headerlink" title="定义items"></a>定义items</h4><p>先打开网页的职位信息页面看看需要提取哪些信息吧</p>
<p><img src="http://okzvvfkr0.bkt.clouddn.com/description.jpg"></p>
<p>除了这些信息外，下面还有职位描述和地址信息等，把这些信息定义为items，即为放置这些信息的容器。<br>打开<code>items.py</code>敲入以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RecruitItem</span>(scrapy.Item):</span><br><span class="line">    position = scrapy.Field()</span><br><span class="line">    company = scrapy.Field()</span><br><span class="line">    salary = scrapy.Field()</span><br><span class="line">    location = scrapy.Field()</span><br><span class="line">    date = scrapy.Field()</span><br><span class="line">    nature = scrapy.Field()</span><br><span class="line">    experience = scrapy.Field()</span><br><span class="line">    education = scrapy.Field()</span><br><span class="line">    number = scrapy.Field()</span><br><span class="line">    category = scrapy.Field()</span><br><span class="line">    description = scrapy.Field()</span><br><span class="line">    address = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>保存后退出。</p>
<h4 id="定义spider"><a href="#定义spider" class="headerlink" title="定义spider"></a>定义spider</h4><p>在<code>spiders</code>文件夹内新建文件<code>recruit_spider.py</code>,这是爬虫的核心文件。<br>首先引入需要的模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> recruit.items <span class="keyword">import</span> RecruitItem</span><br><span class="line"><span class="keyword">from</span> recruit.geturls <span class="keyword">import</span> Geturls</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br></pre></td></tr></table></figure>
<p>这里要引入前面编写的<code>Geturls类</code>和定义的<code>items</code>.<br>新建类，继承框架的爬虫基类，并注明唯一的爬虫名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RecruitSpider</span>(scrapy.Spider):</span><br><span class="line">	name = <span class="string">&#x27;recruit&#x27;</span></span><br></pre></td></tr></table></figure>
<p>其余的方法在这个类中编写。第一个方法是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">	base_url = <span class="string">&quot;http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%E5%A4%A7%E8%BF%9E&amp;kw=php&amp;sm=0&amp;p=1&quot;</span></span><br><span class="line">	geturls = Geturls()</span><br><span class="line">	content = geturls.<span class="built_in">open</span>(base_url)</span><br><span class="line">	suburls = geturls.distill(content)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">for</span> url <span class="keyword">in</span> suburls:</span><br><span class="line">		url_str = <span class="built_in">str</span>(url)</span><br><span class="line">		url_temp = url_str.strip(<span class="string">&quot;&#x27;[]&#x27;&quot;</span>)</span><br><span class="line">		<span class="keyword">yield</span> Request(url_temp, self.parse)</span><br></pre></td></tr></table></figure>
<p>这是<code>scrapy</code>内置的方法名称，依据名称可知是开启<code>requests</code>执行的函数，这里用到了第一步编写的提取url类，直接使用。需要注意的是传过来的url可能包含多余字符，使用<code>strip()</code>去除。<code>base_url</code>是在智联主页搜索后跳转的链接，可以发现这个链接也是有规律可循的，规律是这样的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jl=地点，kw=职位名称，sm=页面状态码(默认=<span class="number">0</span>，可以无视)，p=页码;</span><br></pre></td></tr></table></figure>
<p>可以根据这个规律，自行创建。地点职位页码，根据自身情况修改即可，或者将页码代入循环自动生成一系列地址逐个爬取，这里就不再拓展了……<br>然后将<code>url</code>传给scrapy内置方法<code>Request()</code>，并且使用<code>parse</code>方法作为回调函数进行筛选。</p>
<h4 id="爬取信息"><a href="#爬取信息" class="headerlink" title="爬取信息"></a>爬取信息</h4><p>爬取信息更加简单了，根据<code>xpath</code>获取信息位置取得文本，再剔除多余的空格和字符再保存到<code>items</code>内即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">		items = RecruitItem()</span><br><span class="line"></span><br><span class="line">		items[<span class="string">&#x27;position&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[5]/div[1]/div[1]/h1/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;company&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[5]/div[1]/div[1]/h2/a/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;salary&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[1]/strong/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;location&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[2]/strong/a/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;date&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;//*[@id=&quot;span4freshdate&quot;]/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;nature&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[4]/strong/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;experience&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[5]/strong/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;education&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[6]/strong/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;number&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[7]/strong/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;category&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/ul/li[8]/strong/a/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;description&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/div[1]/div/div[1]/p[1]/text()&#x27;</span>).extract()).strip()</span><br><span class="line">		items[<span class="string">&#x27;address&#x27;</span>] = <span class="string">&quot;&quot;</span>.join(response.xpath(<span class="string">&#x27;/html/body/div[6]/div[1]/div[1]/div/div[1]/h2/text()&#x27;</span>).extract()).strip()</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> items</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="定义pipelines"><a href="#定义pipelines" class="headerlink" title="定义pipelines"></a>定义pipelines</h4><p><code>pipelines</code>模块是将<code>items</code>内的信息做过滤，持久化或其他工作的。<br>这里的代码跟之前<code>SCRAPY简单入门</code>中几乎一样，就不再赘述了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json  </span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RecruitPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">		self.file = codecs.<span class="built_in">open</span>(<span class="string">&#x27;recruit.json&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">		line = json.dumps(OrderedDict(item))</span><br><span class="line">		self.file.write(line.decode(<span class="string">&quot;unicode_escape&quot;</span>))</span><br><span class="line">		self.file.write(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">		<span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">		self.file.close()</span><br></pre></td></tr></table></figure>
<p>将<code>items</code>内的信息写入json文件<br>不要忘记在<code>setting.py</code>文件中，取消以下几行的注释,并修改成自己项目的名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;recruit.pipelines.RecruitPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>OK，所有准备工作完成！打开命令行键入</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl recruit</span><br></pre></td></tr></table></figure>
<p>见证奇迹的发生吧～</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个小项目整体来说并不难，应该说麻烦多一些。其实数据导出之后，才做好了一个开头工作，本打算将这些数据做成一个可视化的图表，这样才能更直观的感受到信息的价值。数据处理稍微查了一下也是个大坑，更何况这种中文信息处理，想想就有点头疼。话不多说，继续学习，待将数据处理完成后写一篇新的文章。</p>

      
    </div>
    
    
    <div class="article-category">
      
      
      
        <b>Tags:</b>
        <a class="article-tag-none-link" href="/tags/Python/" rel="tag">Python</a>
      
    </div>
    
    
  </div>
</article>

  
<nav id="article-nav" class="article-nav">
  
    <a href="/2017/03/30/Composer%E6%90%AD%E5%BB%BAMVC%E6%A1%86%E6%9E%B6/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Composer搭建MVC框架
        
      </div>
    </a>
  
  
    <a href="/2017/01/26/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E8%87%AA%E5%B7%B1%E7%9A%84MVC%E6%A1%86%E6%9E%B6/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          如何编写自己的MVC框架
        
      </div>
    </a>
  
</nav>






    </div>
  </div>
  




<div id="settings-container">
  <div id="dark-mode">dark</div>
  <div id="sans-font">sans</div>
</div>
<script type="text/javascript">
let d=document,r=d.documentElement.style,f=r.setProperty.bind(r),l=localStorage,s=l.getItem('s')||(window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches),n=l.getItem('n'),m=d.getElementById("dark-mode"),b=()=>{f('--bg-color','#fafafa');f('--code-bg-color','#f4f4f4');f('--text-color','#212121');f('--secondary-color','#808080');f('--tertiary-color','#b0b0b0');f('--link-color','#b5c8cf');f('--link-hover-color','#618794');f('--link-bg-color','#dae4e7');f('--selection-color','#dae4e7');m.innerHTML="dark"},c=()=>{f('--bg-color','#212121');f('--code-bg-color','#292929');f('--text-color','#fff');f('--secondary-color','#c0c0c0');f('--tertiary-color','#6e6e6e');f('--link-color','#4d6b75');f('--link-hover-color','#96b1bb');f('--link-bg-color','#5d828e');f('--selection-color','#acc1c9');m.innerHTML="light"},o=d.getElementById("sans-font"),e=()=>{f('--body-stack','"Lora", "Georgia", "Times New Roman", serif');o.innerHTML="sans"},g=()=>{f('--body-stack','"Lato", "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", "Verdana", sans-serif');o.innerHTML="serif"};m.onclick=()=>{if(s==2){s=1;l.setItem('s',s);c()}else{s=2;l.setItem('s',s);b()}};o.onclick=()=>{if(n==2){n=1;l.setItem('n',n);g()}else{n=2;l.setItem('n',n);e()}};if(!s){s=2;l.setItem('s',2)};if(s==1){c()};if(!n){n=2;l.setItem('n',2)};if(n==1){g()};
</script>




</body>
</html>
