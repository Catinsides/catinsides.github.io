<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> SCRAPY 简单入门 · catinsides.github.io</title><meta name="description" content="SCRAPY 简单入门 - catinsides"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://catinsides.github.io/atom.xml" title="catinsides.github.io"><meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="catinsides.github.io" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/Catinsides" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">SCRAPY 简单入门</h1><div class="post-info">2016年12月19日</div><div class="post-content"><blockquote>
<p>SCRAPY是由Python开发的，为爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
</blockquote>
<p>下面通过一个小例子，爬取RS05网站的电影链接，熟悉该框架的基本功能。<br>闲话少叙，下面开始:)</p>
<h3 id="安装SCRAPY"><a href="#安装SCRAPY" class="headerlink" title="安装SCRAPY"></a>安装SCRAPY</h3><p>相信有一半的同学，学习SCRAPY从安装到放弃。<br>我也是折腾了好久，从baidu到google，再到stackoverflow，才完完整整的安装到了电脑上。高兴的我，又在树莓派Raspbian系统上安装了一遍。为了节省大家的时间，这里把Linux和Windows的安装方法都列举出来，方便以后查询。<br>Python版本为2.7</p>
<h4 id="Windows平台："><a href="#Windows平台：" class="headerlink" title="Windows平台："></a>Windows平台：</h4><p>首先到<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/download/details.aspx?id=44266">这里</a>下载VC For Python27<br>然后按照下列顺序安装前置包</p>
<ul>
<li>zope.interface</li>
<li>pyopenssl</li>
<li>twisted</li>
<li>lxml</li>
<li>scrapy</li>
</ul>
<p>不能使用pip下载的，直接google安装包好了。</p>
<h4 id="Linux平台："><a href="#Linux平台：" class="headerlink" title="Linux平台："></a>Linux平台：</h4><p>准确的说应该是Raspbian平台，因为我用的树莓派是XD。相比win平台就简单多了，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get install libxml2-dev libxslt1-dev python-dev</span><br><span class="line">apt-get install python-lxml</span><br><span class="line"><span class="keyword">if</span> error:</span><br><span class="line">pip install pyasn1 --upgrade</span><br></pre></td></tr></table></figure>
<p>好的，大功告成！enjoy！</p>
<h3 id="选择网站"><a href="#选择网站" class="headerlink" title="选择网站"></a>选择网站</h3><p>我真是太懒了，懒到不想点开浏览器去找电影下载。为了节省十几秒找电影的时间，花去了几个小时研究爬虫框架自动爬取链接。想想还是挺值的！<br>这里我选择RS05为目标网站，提取每个电影页面的迅雷url并写入文件，然后直接复制到迅雷里就可以下载了，是不是很简单很开心？</p>
<h3 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h3><p>好的，终于要开始了。<br>CD到项目文件夹，执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject Rs05</span><br></pre></td></tr></table></figure>
<p>随后项目文件夹内变生成了一系列文件，其结构如下：</p>
<blockquote>
<p>Rs05&#x2F;<br>　　scrapy.cfg<br>　　Rs05&#x2F;<br>　　　　<code>__init__.py</code><br>　　　　items.py<br>　　　　piplines.py<br>　　　　settings.py<br>　　　　spiders&#x2F;<br>　　　　　　<code>__init__.py</code><br>　　　　　　…</p>
</blockquote>
<p>简单介绍一下各个文件：<br><code>scrapy.cfg</code> 是项目的配置文件；<br><code>__init__.py</code> 这个文件不用管；<br><code>items.py</code> 文件里编写爬取内容的Field；<br><code>piplines.py</code> 文件里编写爬取后过滤内容的规则或者其他操作，如写入文件；<br><code>settings.py</code> 是项目的设置文件；<br><code>spiders</code> 内放置爬虫文件</p>
<h3 id="设置规则"><a href="#设置规则" class="headerlink" title="设置规则"></a>设置规则</h3><p>首先，需要定义items，可以理解为放置爬取内容的地方，与字典类似。<br>编写方法非常简单，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rs05Item</span>(scrapy.Item):</span><br><span class="line">　　<span class="comment">#name = scrapy.Field()</span></span><br><span class="line">　　title = scrapy.Field()</span><br><span class="line">　　url = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>name根据你的需要随便起名字，数量由爬取种类的多少决定。<br>然后CD到spider文件，新建爬虫文件rs05_spider.py<br>打开文件编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> rs05.items <span class="keyword">import</span> Rs05Item</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rs05Spider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">　　name = <span class="string">&#x27;rs05&#x27;</span></span><br><span class="line">　　start_urls = [<span class="string">&#x27;http://www.rs05.com/&#x27;</span>]</span><br><span class="line">　　rules = (</span><br><span class="line">　　　　Rule(LinkExtractor(allow=(<span class="string">&#x27;[a-z0-9]+\.html&#x27;</span>)),callback=<span class="string">&#x27;parse_item&#x27;</span>),</span><br><span class="line">　　)</span><br><span class="line">　　<span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self,response</span>):			</span><br><span class="line">　　　　items = []</span><br><span class="line">　　　　item = Rs05Item()</span><br><span class="line">　　　　<span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">&#x27;/html/body/div[4]/div[1]/div[1]&#x27;</span>):</span><br><span class="line">　　　　　　title = sel.xpath(<span class="string">&#x27;h1/text()&#x27;</span>).extract()</span><br><span class="line">　　　　　　item[<span class="string">&#x27;title&#x27;</span>] = title <span class="comment">#t.encode(&#x27;utf-8&#x27;) for t in title</span></span><br><span class="line">　　　　　　pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;href=&quot;(.*?)&quot;&#x27;</span>,re.S)			</span><br><span class="line">　　　　　　url = re.findall(pattern, sel.extract())			</span><br><span class="line">　　　　　　item[<span class="string">&#x27;url&#x27;</span>] = url</span><br><span class="line">　　　　　　items.append(item)</span><br><span class="line">　　　　<span class="keyword">return</span> items</span><br></pre></td></tr></table></figure>
<p>根据代码可以看出，爬虫使用了scrapy内置的CrawlSpider，用于跟进链接继续爬取。而linkextractors用于设置跟进链接的规则，使用正则匹配。<code>name</code>为爬虫的名称，该名称唯一。<code>start_urls</code>为开始爬取的链接。需要注意的是，当要设置<code>allowed_domains</code>时，<code>start_urls</code>与<code>allowed_domains</code>不能相同，否则会出现错误。<code>parse_item</code>函数用于解析response的内容，并将结果保存到items中。上述代码使用xpath与re提取内容，当然也可以使用其他方法，比如beautifulsoup。无论用什么方法，最终将需要提取的内容放到items中即可。</p>
<h3 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h3><p>爬取内容后，如何输出呢？<br>此时打开pipelines.py，输入以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rs05Pipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">　　<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">　　　　self.file = codecs.<span class="built_in">open</span>(<span class="string">&#x27;rs05.json&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">　　<span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">　　　　line = json.dumps(OrderedDict(item)) + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">　　　　self.file.write(line.decode(<span class="string">&quot;unicode_escape&quot;</span>))</span><br><span class="line">　　　　<span class="keyword">return</span> item</span><br><span class="line">　　<span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">　　　　self.file.close()</span><br></pre></td></tr></table></figure>
<p>其中,<code>__init__()</code>和<code>close_spider()</code>相当于构造、析构函数，写不写都可以。但是process_item是一定要写的。这里用于处理爬取内容，丢弃或者保存。从上面的代码可以看出，爬取内容保存到了rs05.json文件中。<br>还没有完事，打开settings.py找到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;rs06.pipelines.Rs06Pipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>取消注释，并将名字改为爬虫的名字，后面的数字为优先级，如果你有多个pipelines，可以为它们设定执行顺序。</p>
<h3 id="开始爬取"><a href="#开始爬取" class="headerlink" title="开始爬取"></a>开始爬取</h3><p>好了，一切都设置好之后，CD到项目根目录可以开始爬取了。<br>执行下列命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl Rs05</span><br></pre></td></tr></table></figure>
<p>一大串字母和数字闪过之后，在根目录生成了rs05.json, 打开文件看看是否提取到了想要的内容。我相信多数情况下，都没有一次成功的时候吧……<br>下面介绍两种调试的方法：</p>
<ul>
<li>1.检查spider文件的过滤规则，是否提取到内容。如果使用xpath，可以通过xpath插件，或者chrome shift + i 查看xpath路径；</li>
<li>2.输入命令 <code>scrapy shell &quot;http://xxx&quot;</code> 然后输入 <code>response.selector.xpath(&#39;your rules&#39;)</code>查看内容，验证xpath路径是否正确。</li>
</ul>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>SCRAPY框架适用于中型大型爬虫项目，使我们专注于内容的提取，而不是爬虫的构建。小型项目可以使用urllib,repuests库等等。<br>简单来讲，网络爬虫就是获取网页内容后提取有价值信息的一个技术。我们设置好过滤内容规则后，让电脑自动提取，大大加强了我们提取信息的效率。当然，这一过程中，也会遇到一些困难，如网页改版，封禁IP等等。此时就需要将爬虫升级为机器学习或者分布式爬虫，这里需要更多更深的知识。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2016/12/26/Leetcode.Database-%E9%A2%98%E8%A7%A3%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/" class="prev">上一篇</a><a href="/2016/11/26/CodeIgniter-%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/" class="next">下一篇</a></div><div class="copyright"><p>© 2016 - 2024 <a href="https://catinsides.github.io">catinsides</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>